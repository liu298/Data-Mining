{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mining Frequent Patterns for Each Topic (30pts)\n",
    "In this step, you need to implement a frequent pattern mining algorithm. You can choose whatever frequent pattern mining algorithms you like, such as Apriori, FP-Growth, ECLAT, etc. Note that you need to run your code on 5 files corresponding to 5 topics. The output should be of the form ( [s] (space) [t1 (space) t2 (space) t3 (space) ...] ), where each term should be represented as the term id:\n",
    "#Support          [frequent pattern] \n",
    "#Support          [frequent pattern] \n",
    "...\n",
    " \n",
    "and frequent patterns are sorted from high to low by #Support. Your output files should be put into one directory named as patterns. The i-th file is named as pattern-i.txt.\n",
    "(Hint: you need to figure out min_sup by yourself.)\n",
    "\n",
    "Question to ponder A: How you choose min_sup for this task? Note that we prefer min_sup to be the consistent percentage (e.g. 0.05 / 5%) w.r.t. number of lines in topic files to cope with various-length topic files.\n",
    "Explain how you choose the min_sup in your report. Any reasonable choice will be fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000 0001 \n",
      "0019 \n",
      "0025 0026 0028 0023 0027 0024 \n",
      "0035 0039 0036 0037 0038 \n",
      "0028 0063 0062 \n",
      "0070 0068 0069 0066 0060 0067 \n",
      "0073 0072 0075 \n",
      "0077 0060 0066 0079 0078 0076 \n",
      "0012 0081 0083 \n",
      "0108 0039 \n",
      "   30795\n",
      "   30796\n",
      "   12219\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "!head -10 'topic-0.txt' \n",
    "!cat 'title.txt' | wc -l\n",
    "!cat 'paper.txt' | wc -l\n",
    "!cat 'vocab.txt' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ChooseCand(prePat,n):\n",
    "    \"\"\"\n",
    "    :type prePat: list of tuples\n",
    "    :rtype: list of tuples\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    prePat = sorted(prePat)\n",
    "    for i in range(len(prePat)):\n",
    "        # join step\n",
    "        cur = prePat[i]\n",
    "#         print(cur)\n",
    "        ## find intersections in pattern behind\n",
    "        for j in range(i+1,len(prePat)):\n",
    "            after = prePat[j]\n",
    "            if(isinstance(prePat[0],tuple)):\n",
    "                intersec = set(filter(lambda x: x in cur,after))\n",
    "#                 print(\"intersec\")\n",
    "                if len(intersec)==n-2:\n",
    "                    ## possible to grenerate a new candidate\n",
    "                    new = set(sorted(cur+after))\n",
    "#                     print(new)\n",
    "                    # prune step\n",
    "                    if not hasInfreq(new,prePat) and tuple(new) not in ret:\n",
    "                        ret += [tuple(new)]\n",
    "            else:\n",
    "                ret += [(cur,after)]\n",
    "    return ret\n",
    "\n",
    "def hasInfreq(items,prePat):\n",
    "    for i in items:\n",
    "        sub = items-set(i)\n",
    "        if tuple(sub) not in prePat:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# pre = ['001','002','003']\n",
    "# print(ChooseCand(pre,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AprioriMining(file,n,min_sup):\n",
    "    \"\"\"\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    if n == 1: # base case\n",
    "        patCount = {}\n",
    "        lineCount = 0\n",
    "        for line in file:\n",
    "            lineCount += 1\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if word not in patCount:\n",
    "                    patCount[word]=1\n",
    "                else:\n",
    "                    patCount[word]+=1\n",
    "        for key,val in patCount.items():\n",
    "            if val/lineCount >= min_sup:\n",
    "                ret[key] = val\n",
    "    else:\n",
    "        if n> 1:\n",
    "            prePat = AprioriMining(file,n-1,min_sup).keys()\n",
    "            if len(prePat)!=0:\n",
    "                candids = ChooseCand(prePat,n)\n",
    "                patCount = dict(zip(candids,[0]*len(candids)))\n",
    "                lineCount = 0\n",
    "                for line in file:\n",
    "                    lineCount += 1\n",
    "                    words = set(line.split())\n",
    "                    for candid in candids:\n",
    "                        if set(candid).issubset(words):\n",
    "                            patCount[candid] += 1\n",
    "                for key,val in patCount.items():\n",
    "                    if val/lineCount >= min_sup:\n",
    "                        ret[key] = val\n",
    "    return ret \n",
    "\n",
    "# def test():\n",
    "#     with open('topic-0.txt','r+') as file:\n",
    "#         file = file.readlines()\n",
    "#         print(AprioriMining(file,2,0.01))\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def MiningAllPat(min_sup):\n",
    "    inputFile = [open('topic-{}.txt'.format(i),'r') for i in range(5)]\n",
    "    output = [open('patterns/pattern-{}.txt'.format(i),'w') for i in range(5)]\n",
    "    for t in range(5):\n",
    "        ret = dict()\n",
    "        inputLines = inputFile[t].readlines()\n",
    "        for i in range(9999):\n",
    "            fp = AprioriMining(inputLines,i+1,min_sup)\n",
    "            ret.update(fp)\n",
    "            # print(str(i+1),len(ret))\n",
    "            if len(fp)==0:\n",
    "                break\n",
    "        # sort the support and write to output files\n",
    "        sortKey = sorted(ret,key=ret.get,reverse=True)\n",
    "        for key in sortKey:\n",
    "            if isinstance(key,tuple):\n",
    "                output[t].write(str(ret[key])+' '+' '.join(key)+'\\n')\n",
    "            else:\n",
    "                output[t].write(str(ret[key])+' '+key+'\\n')\n",
    "        inputFile[t].close()\n",
    "        output[t].close()\n",
    "    \n",
    "print(MiningAll(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 63\n",
      "2 21\n",
      "3 0\n",
      "1 69\n",
      "2 8\n",
      "3 0\n",
      "1 65\n",
      "2 10\n",
      "3 0\n",
      "1 61\n",
      "2 9\n",
      "3 0\n",
      "1 54\n",
      "2 5\n",
      "3 0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def MiningMaxPat(min_sup):\n",
    "    inputFile = [open('topic-{}.txt'.format(i),'r') for i in range(5)]\n",
    "    output = [open('max/max-{}.txt'.format(i),'w') for i in range(5)]\n",
    "    for t in range(5):\n",
    "        inputLines = inputFile[t].readlines()\n",
    "        ret = dict()\n",
    "        for i in range(9999):\n",
    "            # find i-itemset patterns\n",
    "            fp = AprioriMining(inputLines,i+1,min_sup)\n",
    "            # print(str(i+1),len(fp))\n",
    "            if len(fp)==0:\n",
    "                # find max pattern\n",
    "                ret = prePat\n",
    "                break\n",
    "            else:\n",
    "                prePat = fp\n",
    "        # sort the support and write to output files\n",
    "        sortKey = sorted(ret,key=ret.get,reverse=True)\n",
    "        for key in sortKey:\n",
    "            if isinstance(key,tuple):\n",
    "                output[t].write(str(ret[key])+' '+' '.join(key)+'\\n')\n",
    "            else:\n",
    "                output[t].write(str(ret[key])+' '+key+'\\n')\n",
    "        inputFile[t].close()\n",
    "        output[t].close()\n",
    "    \n",
    "print(MiningMaxPat(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 63\n",
      "2 21\n",
      "3 0\n",
      "1 69\n",
      "2 8\n",
      "3 0\n",
      "1 65\n",
      "2 10\n",
      "3 0\n",
      "1 61\n",
      "2 9\n",
      "3 0\n",
      "1 54\n",
      "2 5\n",
      "3 0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def MiningClosedPat(min_sup):\n",
    "    inputFile = [open('topic-{}.txt'.format(i),'r') for i in range(5)]\n",
    "    output = [open('closed/closed-{}.txt'.format(i),'w') for i in range(5)]\n",
    "    for t in range(5):\n",
    "        ret = dict()\n",
    "        inputLines = inputFile[t].readlines()\n",
    "        for i in range(9999):\n",
    "            # find (i+1)-itemset patterns\n",
    "            fp = AprioriMining(inputLines,i+1,min_sup)\n",
    "            # print(str(i+1),len(fp))\n",
    "            if len(fp)==0:\n",
    "                break\n",
    "            else:\n",
    "                for preK,preV in ret.items():\n",
    "                    for curK,curV in fp.items():\n",
    "                        if set(preK).issubset(set(curK)) and preV==curV:\n",
    "                            del ret[preK]\n",
    "                ret.update(fp)\n",
    "        # sort the support and write to output files\n",
    "        sortKey = sorted(ret,key=ret.get,reverse=True)\n",
    "        for key in sortKey:\n",
    "            if isinstance(key,tuple):\n",
    "                output[t].write(str(ret[key])+' '+' '.join(key)+'\\n')\n",
    "            else:\n",
    "                output[t].write(str(ret[key])+' '+key+'\\n')\n",
    "        inputFile[t].close()\n",
    "        output[t].close()\n",
    "    \n",
    "print(MiningClosedPat(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 111, 2: 22, 3: 333, 4: 444, 5: 555}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x={1:111,3:333,4:444,2:22}\n",
    "y={1:111,5:555}\n",
    "# sorted(x,key=x.get)\n",
    "y.update(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
